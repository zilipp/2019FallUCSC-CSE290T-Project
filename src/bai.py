# -*- coding: utf-8 -*-
"""Copy of CSE290-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CdYHp8RJtlFZ2sl8kPgcT3oHWSBMvWTe
"""

import re
import nltk
import pandas as pd
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

class Processor:
  def __init__(self):
    self._df = pd.DataFrame()
  
  def _load_data(self, file_name):
    try:
      self._df = pd.read_csv(file_name)
    except OSError:
      print("The file '{}' is not found!".format(file_name))

  def _manage_noisy_data(self):
    self._df = self._df.dropna()

  def _extract_features(self): 
    def _get_documents(column_name):
      self._df[column_name] = self._df['title'] + self._df['text']                       
      self._df[column_name] = self._df[column_name].fillna('')
      self._df[column_name] = self._df[column_name].apply(lambda x : _tokenize(x))
      return self._df[column_name].tolist() # get the text column 
    def _tokenize(text):
      text = text.lower() # lowercase
      text = re.sub("<!--?.*?-->","",text) #remove tags
      text = re.sub("(\\d|\\W)+"," ",text) # remove special characters and digits
      return text
    def _get_tfidf_models(documents):
      cv = CountVectorizer(max_df = 0.85, stop_words = _get_stop_words("stop_words.txt"), max_features = 10000)
      tfidf_transformer = TfidfTransformer(smooth_idf = True, use_idf = True)
      word_count_vector = cv.fit_transform(documents)
      tfidf_transformer.fit(word_count_vector) 
      #print(cv.get_feature_names())
      #print(word_count_vector.toarray())
      return cv, tfidf_transformer
    def _get_stop_words(file_name): # load stop words
      try: 
        with open(file_name, 'r', encoding="utf-8") as f:
          stopwords = f.readlines()
          stop_set = set(m.strip() for m in stopwords)
          return frozenset(stop_set)
      except OSError:
        print("The file '{}' is not found!".format(file_name))
    def _show_features(cv, tiidf_transformer, curr_doc):
      feature_names = cv.get_feature_names() # you only needs to do this once, this is a mapping of index to 
      tfidf_vector = tfidf_transformer.transform(cv.transform([curr_doc])) #generate tf-idf for the given document
      sorted_items = _sort_coo(tfidf_vector.tocoo()) #sort the tf-idf vectors by descending order of scores
      keywords = _extract_top_n_from_vector(feature_names, sorted_items, 10) #extract only the top n; n here is 10
      print("\n=====Doc=====") # now print the results
      print(curr_doc)
      print("\n===Keywords===")
      for k in keywords:
          print(k, keywords[k])
    def _sort_coo(coo_matrix):
      tuples = zip(coo_matrix.col, coo_matrix.data)
      return sorted(tuples, key=lambda x: (x[1], x[0]), reverse = True)
    def _extract_top_n_from_vector(feature_names, sorted_items, topn = 10):
      """get the feature names and tf-idf score of top n items"""
      sorted_items = sorted_items[: topn] #use only topn items from vector
      score_vals = []
      feature_vals = []
      for idx, score in sorted_items: # word index and corresponding tf-idf score
        score_vals.append(round(score, 3))
        feature_vals.append(feature_names[idx])  #keep track of feature name and its corresponding score
      results= {} #create a tuples of feature,score
      for idx in range(len(feature_vals)): #results = zip(feature_vals,score_vals)
        results[feature_vals[idx]] = score_vals[idx]
      return results
    documents = _get_documents('document')
    cv, tfidf_transformer = _get_tfidf_models(documents)
    for i in range(10):
      _show_features(cv, tfidf_transformer, documents[i*100])

  def get_df(self):
    return self._df

  def summary(self, instruction = None):
    def overview():
      print(self._df.info())
    def null_sum():
      print(self._df.isnull().sum())
    def null_rows():
      print(self._df[self._df.isna().any(axis=1)])
    def null_rows_indices():
      for col in self._df.columns:
        print(col)
        print(self._df[self._df[col].isnull()].index.tolist())
    if (instruction == "overview"):
      overview()
    elif (instruction == "null_sum"):
      null_sum()
    elif (instruction == "null_rows"):
      null_rows()
    elif (instruction == "null_rows_indices"):
      null_rows_indices()

  def process(self, file_name):
    self._load_data(file_name)
    #self._manage_noisy_data()
    self._extract_features()

class Learner:
  def __init__(self):
    pass
  
class Developer:
  def __init__(self):
    pass

if __name__ == "__main__":
  processor = Processor()
  processor.process("fake.csv")

